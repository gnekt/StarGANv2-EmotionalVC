{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mConfigs\u001b[0m/            \u001b[01;34mUtils\u001b[0m/          download.py    requirements.txt\n",
      "\u001b[01;34mData\u001b[0m/               \u001b[01;34m__pycache__\u001b[0m/    inference.py   train.ipynb\n",
      "\u001b[01;34mEnumeratorFactory\u001b[0m/  \u001b[01;34mdataset\u001b[0m/        losses.py      train.py\n",
      "LICENSE             dataset.zip     meldataset.py  trainer.py\n",
      "\u001b[01;34mModels\u001b[0m/             \u001b[01;34mdataset_maker\u001b[0m/  models.py      transforms.py\n",
      "README.md           download.ipynb  optimizers.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SoundFile\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting parallel_wavegan\n",
      "  Downloading parallel_wavegan-0.5.5.tar.gz (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (5.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (8.1.3)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from SoundFile) (1.15.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from munch) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from parallel_wavegan) (1.12.0+cu116)\n",
      "Requirement already satisfied: setuptools>=38.5.1 in /usr/local/lib/python3.9/dist-packages (from parallel_wavegan) (63.1.0)\n",
      "Collecting tensorboardX>=1.8\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from parallel_wavegan) (3.5.2)\n",
      "Requirement already satisfied: tqdm>=4.26.1 in /usr/local/lib/python3.9/dist-packages (from parallel_wavegan) (4.64.0)\n",
      "Collecting kaldiio>=2.14.1\n",
      "  Downloading kaldiio-2.17.2.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from parallel_wavegan) (3.7.0)\n",
      "Collecting yq>=2.10.0\n",
      "  Downloading yq-3.1.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (from parallel_wavegan) (4.5.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from parallel_wavegan) (3.7.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa) (5.1.1)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.8.1)\n",
      "Collecting numba>=0.45.1\n",
      "  Downloading numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.23.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->SoundFile) (2.21)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.1.0->parallel_wavegan) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.1.0->parallel_wavegan) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.1.0->parallel_wavegan) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.1.0->parallel_wavegan) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.1.0->parallel_wavegan) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.1.0->parallel_wavegan) (2.8.2)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX>=1.8->parallel_wavegan) (3.19.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->parallel_wavegan) (4.3.0)\n",
      "Collecting toml>=0.10.0\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting xmltodict>=0.11.0\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting argcomplete>=1.8.1\n",
      "  Downloading argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown->parallel_wavegan) (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown->parallel_wavegan) (2.3.2.post1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.7.1)\n",
      "Building wheels for collected packages: parallel_wavegan, audioread, kaldiio\n",
      "  Building wheel for parallel_wavegan (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for parallel_wavegan: filename=parallel_wavegan-0.5.5-py3-none-any.whl size=72444 sha256=a9890f93be2b7056fefe1f455a59bef04c8cf9ed4a9f17e193f4ad5868172f95\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/52/22/a808b1f340ad33263b8998d6945d8013c7c4944ba0a7d5afc9\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23702 sha256=a66ca0bf2bc6bb33f228a83725b6dcd6b15c29bdf2c4054edbecfc0eb996c5a2\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/76/a4/cfb55573167a1f5bde7d7a348e95e509c64b2c3e8f921932c3\n",
      "  Building wheel for kaldiio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaldiio: filename=kaldiio-2.17.2-py3-none-any.whl size=24447 sha256=e4083b369729970db691f89d7de14c81c14b490477409ae154889ba2eeb2c547\n",
      "  Stored in directory: /root/.cache/pip/wheels/5d/8b/4e/17aac05e86a04db23f508115dc5aa24a6077fa5357eaee0d78\n",
      "Successfully built parallel_wavegan audioread kaldiio\n",
      "Installing collected packages: pydub, appdirs, xmltodict, toml, tensorboardX, munch, llvmlite, kaldiio, audioread, argcomplete, yq, SoundFile, pooch, numba, resampy, librosa, parallel_wavegan\n",
      "Successfully installed SoundFile-0.11.0 appdirs-1.4.4 argcomplete-2.0.0 audioread-3.0.0 kaldiio-2.17.2 librosa-0.9.2 llvmlite-0.39.1 munch-2.5.0 numba-0.56.4 parallel_wavegan-0.5.5 pooch-1.6.0 pydub-0.25.1 resampy-0.4.2 tensorboardX-2.5.1 toml-0.10.2 xmltodict-0.13.0 yq-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install SoundFile munch parallel_wavegan pydub pyyaml click librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import click\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from functools import reduce\n",
    "from munch import Munch\n",
    "\n",
    "from meldataset import build_dataloader\n",
    "from optimizers import build_optimizer\n",
    "from models import build_model\n",
    "from trainer import Trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from Utils.ASR.models import ASRCNN\n",
    "from Utils.JDC.model import JDCNet\n",
    "\n",
    "import logging\n",
    "from logging import StreamHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'log_dir': 'Models/Experiment-2', 'save_freq': 10, 'device': 'cuda', 'epochs': 150, 'batch_size': 5, 'pretrained_model': '', 'load_only_params': False, 'fp16_run': True, 'dataset_configuration': {'data_separetor': '|', 'data_header': ['actor_id', 'statement_id', 'source_path', 'source_emotion', 'reference_path', 'reference_emotion']}, 'F0_path': 'Utils/JDC/bst.t7', 'ASR_config': 'Utils/ASR/config.yml', 'ASR_path': 'Utils/ASR/epoch_00100.pth', 'preprocess_params': {'sr': 24000, 'spect_params': {'n_fft': 2048, 'win_length': 1200, 'hop_length': 300}}, 'model_params': {'dim_in': 64, 'style_dim': 64, 'latent_dim': 16, 'num_domains': 4, 'max_conv_dim': 512, 'n_repeat': 4, 'w_hpf': 0, 'F0_channel': 256}, 'loss_params': {'g_loss': {'lambda_sty': 1.0, 'lambda_cyc': 5.0, 'lambda_ds': 1.0, 'lambda_norm': 1.0, 'lambda_asr': 10.0, 'lambda_f0': 5.0, 'lambda_f0_sty': 0.1, 'lambda_adv': 2.0, 'lambda_adv_cls': 0.5, 'norm_bias': 0.5}, 'd_loss': {'lambda_reg': 1.0, 'lambda_adv_cls': 0.1, 'lambda_con_reg': 10.0}, 'adv_cls_epoch': 50, 'con_reg_epoch': 30}, 'optimizer_params': {'lr': 0.0001}}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 150, 'steps_per_epoch': 3632}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 150, 'steps_per_epoch': 3632}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 150, 'steps_per_epoch': 3632}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 150, 'steps_per_epoch': 3632}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 150, 'steps_per_epoch': 3632}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 150, 'steps_per_epoch': 3632}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train]:   0%|          | 0/3632 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m             trainer\u001b[39m.\u001b[39msave_checkpoint(osp\u001b[39m.\u001b[39mjoin(log_dir, \u001b[39m'\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m%05d\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m epoch))\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 110\u001b[0m main(\u001b[39m\"\u001b[39;49m\u001b[39mConfigs/config.yml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [7], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     93\u001b[0m     epoch \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mepochs\n\u001b[0;32m---> 94\u001b[0m     train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49m_train_epoch()\n\u001b[1;32m     95\u001b[0m     eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39m_eval_epoch()\n\u001b[1;32m     96\u001b[0m     results \u001b[39m=\u001b[39m train_results\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/tesi/StarGANv2-EmotionalVC/trainer.py:162\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m use_con_reg \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mcon_reg_epoch)\n\u001b[1;32m    160\u001b[0m use_adv_cls \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39madv_cls_epoch)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mfor\u001b[39;00m train_steps_per_epoch, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[train]\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m1\u001b[39m):\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[39m### load data\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     batch \u001b[39m=\u001b[39m [b\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch]\n\u001b[1;32m    166\u001b[0m     x_real, y_org, x_ref, x_ref2, y_trg, z_trg, z_trg2 \u001b[39m=\u001b[39m batch\n",
      "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1272\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1271\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1272\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1273\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1274\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    180\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True #\n",
    "\n",
    "def main(config_path):\n",
    "    config = yaml.safe_load(open(config_path))\n",
    "    print(config)\n",
    "    log_dir = config['log_dir']\n",
    "    if not osp.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n",
    "    shutil.copy(config_path, osp.join(log_dir, osp.basename(config_path)))\n",
    "    writer = SummaryWriter(log_dir + \"/tensorboard\")\n",
    "\n",
    "    # write logs\n",
    "    file_handler = logging.FileHandler(osp.join(log_dir, 'train.log'))\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(logging.Formatter('%(levelname)s:%(asctime)s: %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    ### Get configuration\n",
    "    batch_size = config.get('batch_size', 2)\n",
    "    device = config.get('device', 'cpu')\n",
    "    epochs = config.get('epochs', 1000)\n",
    "    save_freq = config.get('save_freq', 20)\n",
    "    dataset_configuration = config.get('dataset_configuration', None)\n",
    "    stage = config.get('stage', 'star')\n",
    "    fp16_run = config.get('fp16_run', False)\n",
    "    ###\n",
    "    \n",
    "    train_set_path = \"./Data/training_list.txt\"\n",
    "    validation_set_path = \"./Data/validation_list.txt\"\n",
    "    # load dataloader \n",
    "    train_dataloader = build_dataloader(train_set_path,dataset_configuration,\n",
    "                                        batch_size=batch_size,\n",
    "                                        num_workers=2,\n",
    "                                        device=device)\n",
    "    \n",
    "    val_dataloader = build_dataloader(validation_set_path,dataset_configuration,\n",
    "                                        batch_size=batch_size,\n",
    "                                        num_workers=2,\n",
    "                                        device=device)\n",
    "\n",
    "    # load pretrained ASR model, FROZEN\n",
    "    ASR_config = config.get('ASR_config', False)\n",
    "    ASR_path = config.get('ASR_path', False)\n",
    "    with open(ASR_config) as f:\n",
    "            ASR_config = yaml.safe_load(f)\n",
    "    ASR_model_config = ASR_config['model_params']\n",
    "    ASR_model = ASRCNN(**ASR_model_config)\n",
    "    params = torch.load(ASR_path, map_location='cpu')['model']\n",
    "    ASR_model.load_state_dict(params)\n",
    "    _ = ASR_model.eval()    \n",
    "    \n",
    "    # load pretrained F0 model\n",
    "    F0_path = config.get('F0_path', False)\n",
    "    F0_model = JDCNet(num_class=1, seq_len=192)\n",
    "    params = torch.load(F0_path, map_location='cpu')['net']\n",
    "    F0_model.load_state_dict(params)\n",
    "    \n",
    "    # build model\n",
    "    model, model_ema = build_model(Munch(config['model_params']), F0_model, ASR_model)\n",
    "\n",
    "    scheduler_params = {\n",
    "        \"max_lr\": float(config['optimizer_params'].get('lr', 2e-4)),\n",
    "        \"pct_start\": float(config['optimizer_params'].get('pct_start', 0.0)),\n",
    "        \"epochs\": epochs,\n",
    "        \"steps_per_epoch\": len(train_dataloader),\n",
    "    }\n",
    "    \n",
    "    _ = [model[key].to(device) for key in model]\n",
    "    _ = [model_ema[key].to(device) for key in model_ema]\n",
    "    scheduler_params_dict = {key: scheduler_params.copy() for key in model}\n",
    "    optimizer = build_optimizer({key: model[key].parameters() for key in model},\n",
    "                                      scheduler_params_dict=scheduler_params_dict)\n",
    "\n",
    "    trainer = Trainer(args=Munch(config['loss_params']), model=model,\n",
    "                            model_ema=model_ema,\n",
    "                            optimizer=optimizer,\n",
    "                            device=device,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            val_dataloader=val_dataloader,\n",
    "                            logger=logger,\n",
    "                            fp16_run=fp16_run)\n",
    "\n",
    "    if config.get('pretrained_model', '') != '':\n",
    "        trainer.load_checkpoint(config['pretrained_model'],\n",
    "                                load_only_params=config.get('load_only_params', True))\n",
    "\n",
    "    for _ in range(1, epochs+1):\n",
    "        epoch = trainer.epochs\n",
    "        train_results = trainer._train_epoch()\n",
    "        eval_results = trainer._eval_epoch()\n",
    "        results = train_results.copy()\n",
    "        results.update(eval_results)\n",
    "        logger.info('--- epoch %d ---' % epoch)\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, float):\n",
    "                logger.info('%-15s: %.4f' % (key, value))\n",
    "                writer.add_scalar(key, value, epoch)\n",
    "            else:\n",
    "                for v in value:\n",
    "                    writer.add_figure('eval_spec', v, epoch)\n",
    "        if (epoch % save_freq) == 0:\n",
    "            trainer.save_checkpoint(osp.join(log_dir, 'epoch_%05d.pth' % epoch))\n",
    "    return 0\n",
    "\n",
    "main(\"Configs/config.yml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('stargan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "af3ab013e161d7e2071e405d816695dd1dca74efb3c18ca57ddcf9213a3f1219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
