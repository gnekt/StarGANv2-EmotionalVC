{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwaQq4GRU_Nw"
      },
      "source": [
        "# StarGANv2-VC Demo (VCTK 20 Speakers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCpoXuZeGKAn"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/media/users/dimaio/tesi/StarGANv2-EmotionalVC\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3on9IjGhVGTP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/users/dimaio/miniconda3/envs/stargan/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# load packages\n",
        "import random\n",
        "import yaml\n",
        "from munch import Munch\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import librosa\n",
        "\n",
        "from Utils.ASR.models import ASRCNN\n",
        "from Utils.JDC.model import JDCNet\n",
        "from models import Generator, MappingNetwork, StyleEncoder\n",
        "from dataset_maker.emotion_mapping import emotion_map\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4SFZgbD4FiHY"
      },
      "outputs": [],
      "source": [
        "# Source: http://speech.ee.ntu.edu.tw/~jjery2243542/resource/model/is18/en_speaker_used.txt\n",
        "# Source: https://github.com/jjery2243542/voice_conversion\n",
        "\n",
        "emotion_map\n",
        "\n",
        "emotions_label = [emotion_value for emotion_id, emotion_value in emotion_map.items() if emotion_id != \"neutral\"]\n",
        "emotions_text = [emotion_id for emotion_id, emotion_value in emotion_map.items() if emotion_id != \"neutral\"]\n",
        "to_mel = torchaudio.transforms.MelSpectrogram(\n",
        "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
        "mean, std = -4, 4\n",
        "\n",
        "def preprocess(wave):\n",
        "    wave_tensor = torch.from_numpy(wave).float()\n",
        "    mel_tensor = to_mel(wave_tensor)\n",
        "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
        "    return mel_tensor\n",
        "\n",
        "def build_model(model_params={}):\n",
        "    args = Munch(model_params)\n",
        "    generator = Generator(args.dim_in, args.style_dim, args.max_conv_dim, w_hpf=args.w_hpf, F0_channel=args.F0_channel)\n",
        "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, hidden_dim=args.max_conv_dim)\n",
        "    style_encoder = StyleEncoder(args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim)\n",
        "    \n",
        "    nets_ema = Munch(generator=generator,\n",
        "                     mapping_network=mapping_network,\n",
        "                     style_encoder=style_encoder)\n",
        "\n",
        "    return nets_ema\n",
        "\n",
        "def compute_style(speaker_dicts):\n",
        "    reference_embeddings = {}\n",
        "    for key, (path, speaker) in speaker_dicts.items():\n",
        "        if path == \"\":\n",
        "            label = torch.LongTensor([speaker]).to('cuda:1')\n",
        "            latent_dim = starganv2.mapping_network.shared[0].in_features\n",
        "            ref = starganv2.mapping_network(torch.randn(1, latent_dim).to('cuda:1'), label)\n",
        "        else:\n",
        "            wave, sr = librosa.load(path, sr=24000)\n",
        "            audio, index = librosa.effects.trim(wave, top_db=30)\n",
        "            if sr != 24000:\n",
        "                wave = librosa.resample(wave, sr, 24000)\n",
        "            mel_tensor = preprocess(wave).to('cuda:1')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                label = torch.LongTensor([speaker])\n",
        "                ref = starganv2.style_encoder(mel_tensor.unsqueeze(1), label)\n",
        "        reference_embeddings[key] = (ref, label)\n",
        "    \n",
        "    return reference_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWjcdaVFiHZ"
      },
      "source": [
        "### Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qCeDZv_NFiHZ"
      },
      "outputs": [],
      "source": [
        "# load F0 model\n",
        "\n",
        "F0_model = JDCNet(num_class=1, seq_len=192)\n",
        "params = torch.load(\"Utils/JDC/bst.t7\")['net']\n",
        "F0_model.load_state_dict(params)\n",
        "_ = F0_model.eval()\n",
        "F0_model = F0_model.to('cuda:1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NZA3ot-oF5t-"
      },
      "outputs": [],
      "source": [
        "# load vocoder\n",
        "from parallel_wavegan.utils import load_model\n",
        "vocoder = load_model(\"Vocoder/PreTrainedVocoder/checkpoint-400000steps.pkl\").to('cuda:1').eval()\n",
        "vocoder.remove_weight_norm()\n",
        "_ = vocoder.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ou4367LCyefA"
      },
      "outputs": [],
      "source": [
        "# load starganv2\n",
        "\n",
        "model_path = 'Models/Experiment-2/epoch_00100.pth'\n",
        "\n",
        "with open('Models/Experiment-2/config.yml') as f:\n",
        "    starganv2_config = yaml.safe_load(f)\n",
        "starganv2 = build_model(model_params=starganv2_config[\"model_params\"])\n",
        "params = torch.load(model_path, map_location='cpu')\n",
        "params = params['model_ema']\n",
        "_ = [starganv2[key].load_state_dict(params[key]) for key in starganv2]\n",
        "_ = [starganv2[key].eval() for key in starganv2]\n",
        "starganv2.style_encoder = starganv2.style_encoder.to('cuda:1')\n",
        "starganv2.mapping_network = starganv2.mapping_network.to('cuda:1')\n",
        "starganv2.generator = starganv2.generator.to('cuda:1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLk7iesuFiHb"
      },
      "source": [
        "### Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-BTWFhHFiHb",
        "outputId": "651dcd1e-1c4a-4665-eee7-4ebf42db3b2b"
      },
      "outputs": [],
      "source": [
        "# load input wave\n",
        "wav_path = 'Demo/neutral.wav'\n",
        "audio, source_sr = librosa.load(wav_path, sr=24000)\n",
        "audio = audio / np.max(np.abs(audio))\n",
        "audio.dtype = np.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KvrzOfuFiHc"
      },
      "source": [
        "#### Convert by style encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uf5t5_pIFiHc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neutral': 0, 'anger': 1, 'happy': 2, 'sad': 3}\n",
            "neutral\n",
            "anger\n",
            "happy\n",
            "sad\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_convolution)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m index\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     emotion_ref[val] \u001b[39m=\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDemo/emotion_sample/\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m.wav\u001b[39m\u001b[39m'\u001b[39m, val)\n\u001b[0;32m----> 9\u001b[0m reference_embeddings \u001b[39m=\u001b[39m compute_style(emotion_ref)\n",
            "Cell \u001b[0;32mIn [3], line 46\u001b[0m, in \u001b[0;36mcompute_style\u001b[0;34m(speaker_dicts)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     45\u001b[0m             label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([speaker])\n\u001b[0;32m---> 46\u001b[0m             ref \u001b[39m=\u001b[39m starganv2\u001b[39m.\u001b[39;49mstyle_encoder(mel_tensor\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), label)\n\u001b[1;32m     47\u001b[0m     reference_embeddings[key] \u001b[39m=\u001b[39m (ref, label)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m reference_embeddings\n",
            "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/tesi/StarGANv2-EmotionalVC/models.py:296\u001b[0m, in \u001b[0;36mStyleEncoder.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[0;32m--> 296\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared(x)\n\u001b[1;32m    298\u001b[0m     h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mview(h\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    299\u001b[0m     out \u001b[39m=\u001b[39m []\n",
            "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/miniconda3/envs/stargan/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_convolution)"
          ]
        }
      ],
      "source": [
        "# with reference, using style encoder\n",
        "emotion_ref={}\n",
        "print(emotion_map)\n",
        "for index,val in emotion_map.items():\n",
        "    print(index)\n",
        "    if index==\"neutral\": continue\n",
        "    emotion_ref[val] = (f'Demo/emotion_sample/{index}/{index}.wav', val)\n",
        "\n",
        "reference_embeddings = compute_style(emotion_ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T5tahObUyN-d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# conversion \n",
        "import time\n",
        "start = time.time()\n",
        "    \n",
        "source = preprocess(audio).to('cuda:1')\n",
        "keys = []\n",
        "converted_samples = {}\n",
        "reconstructed_samples = {}\n",
        "converted_mels = {}\n",
        "\n",
        "for key, (ref, _) in reference_embeddings.items():\n",
        "    with torch.no_grad():\n",
        "        f0_feat = F0_model.get_feature_GAN(source.unsqueeze(1))\n",
        "        out = starganv2.generator(source.unsqueeze(1), ref, F0=f0_feat)\n",
        "        \n",
        "        c = out.transpose(-1, -2).squeeze().to('cuda:1')\n",
        "        y_out = vocoder.inference(c)\n",
        "        y_out = y_out.view(-1).cpu()\n",
        "\n",
        "        if key not in speaker_dicts or speaker_dicts[key][0] == \"\":\n",
        "            recon = None\n",
        "        else:\n",
        "            wave, sr = librosa.load(speaker_dicts[key][0], sr=24000)\n",
        "            mel = preprocess(wave)\n",
        "            c = mel.transpose(-1, -2).squeeze().to('cuda:1')\n",
        "            recon = vocoder.inference(c)\n",
        "            recon = recon.view(-1).cpu().numpy()\n",
        "\n",
        "    converted_samples[key] = y_out.numpy()\n",
        "    reconstructed_samples[key] = recon\n",
        "\n",
        "    converted_mels[key] = out\n",
        "    \n",
        "    keys.append(key)\n",
        "end = time.time()\n",
        "print('total processing time: %.3f sec' % (end - start) )\n",
        "\n",
        "import IPython.display as ipd\n",
        "for key, wave in converted_samples.items():\n",
        "    print('Converted: %s' % key)\n",
        "    display(ipd.Audio(wave, rate=24000))\n",
        "    print('Reference (vocoder): %s' % key)\n",
        "    if reconstructed_samples[key] is not None:\n",
        "        display(ipd.Audio(reconstructed_samples[key], rate=24000))\n",
        "\n",
        "print('Original (vocoder):')\n",
        "wave, sr = librosa.load(wav_path, sr=24000)\n",
        "mel = preprocess(wave)\n",
        "c = mel.transpose(-1, -2).squeeze().to('cuda:1')\n",
        "with torch.no_grad():\n",
        "    recon = vocoder.inference(c)\n",
        "    recon = recon.view(-1).cpu().numpy()\n",
        "display(ipd.Audio(recon, rate=24000))\n",
        "print('Original:')\n",
        "display(ipd.Audio(wav_path, rate=24000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWh3o9hvGvJt"
      },
      "source": [
        "#### Convert by mapping network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb_9D2q4FiHd"
      },
      "outputs": [],
      "source": [
        "# no reference, using mapping network\n",
        "speaker_dicts = {}\n",
        "selected_speakers = [228,230]\n",
        "for s in selected_speakers:\n",
        "    k = s\n",
        "    speaker_dicts['p' + str(s)] = ('', speakers.index(s))\n",
        "\n",
        "reference_embeddings = compute_style(speaker_dicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf65lUv5FiHd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# conversion \n",
        "import time\n",
        "start = time.time()\n",
        "    \n",
        "source = preprocess(audio).to('cuda:0')\n",
        "keys = []\n",
        "converted_samples = {}\n",
        "reconstructed_samples = {}\n",
        "converted_mels = {}\n",
        "\n",
        "for key, (ref, _) in reference_embeddings.items():\n",
        "    with torch.no_grad():\n",
        "        f0_feat = F0_model.get_feature_GAN(source.unsqueeze(1))\n",
        "        out = starganv2.generator(source.unsqueeze(1), ref, F0=f0_feat)\n",
        "        \n",
        "        c = out.transpose(-1, -2).squeeze().to('cuda')\n",
        "        y_out = vocoder.inference(c)\n",
        "        y_out = y_out.view(-1).cpu()\n",
        "\n",
        "        if key not in speaker_dicts or speaker_dicts[key][0] == \"\":\n",
        "            recon = None\n",
        "        else:\n",
        "            wave, sr = librosa.load(speaker_dicts[key][0], sr=24000)\n",
        "            mel = preprocess(wave)\n",
        "            c = mel.transpose(-1, -2).squeeze().to('cuda')\n",
        "            recon = vocoder.inference(c)\n",
        "            recon = recon.view(-1).cpu().numpy()\n",
        "\n",
        "    converted_samples[key] = y_out.numpy()\n",
        "    reconstructed_samples[key] = recon\n",
        "\n",
        "    converted_mels[key] = out\n",
        "    \n",
        "    keys.append(key)\n",
        "end = time.time()\n",
        "print('total processing time: %.3f sec' % (end - start) )\n",
        "\n",
        "import IPython.display as ipd\n",
        "for key, wave in converted_samples.items():\n",
        "    print('Converted: %s' % key)\n",
        "    display(ipd.Audio(wave, rate=24000))\n",
        "    print('Reference (vocoder): %s' % key)\n",
        "    if reconstructed_samples[key] is not None:\n",
        "        display(ipd.Audio(reconstructed_samples[key], rate=24000))\n",
        "\n",
        "print('Original (vocoder):')\n",
        "wave, sr = librosa.load(wav_path, sr=24000)\n",
        "mel = preprocess(wave)\n",
        "c = mel.transpose(-1, -2).squeeze().to('cuda')\n",
        "with torch.no_grad():\n",
        "    recon = vocoder.inference(c)\n",
        "    recon = recon.view(-1).cpu().numpy()\n",
        "display(ipd.Audio(recon, rate=24000))\n",
        "print('Original:')\n",
        "display(ipd.Audio(wav_path, rate=24000))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SWh3o9hvGvJt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('stargan')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "af3ab013e161d7e2071e405d816695dd1dca74efb3c18ca57ddcf9213a3f1219"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
