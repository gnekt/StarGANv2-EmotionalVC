{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwaQq4GRU_Nw"
      },
      "source": [
        "# Emotion Embedding Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCpoXuZeGKAn"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3on9IjGhVGTP"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mL'esecuzione di celle con 'Python 3.6.9 64-bit' richiede il pacchetto ipykernel.\n",
            "\u001b[1;31mEseguire il comando seguente per installare 'ipykernel' nell'ambiente Python. \n",
            "\u001b[1;31mComando: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# load packages\n",
        "import random\n",
        "import yaml\n",
        "from munch import Munch\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import librosa\n",
        "import pandas \n",
        "from models import StyleEncoder\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "DATA_PATH = \"Data/emotion_embedding_test_file.txt\"\n",
        "DATA_HEADER = [\"file_path\",\"emotion_id\"]\n",
        "DATA_SEPARETOR = \"|\"\n",
        "MODEL_PATH = \"Models/Experiment-1\"\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4SFZgbD4FiHY"
      },
      "outputs": [],
      "source": [
        "# Source: http://speech.ee.ntu.edu.tw/~jjery2243542/resource/model/is18/en_speaker_used.txt\n",
        "# Source: https://github.com/jjery2243542/voice_conversion\n",
        "\n",
        "to_mel = torchaudio.transforms.MelSpectrogram(\n",
        "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
        "mean, std = -4, 4\n",
        "\n",
        "def preprocess(wave):\n",
        "    wave_tensor = torch.from_numpy(wave).float()\n",
        "    mel_tensor = to_mel(wave_tensor)\n",
        "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
        "    return mel_tensor\n",
        "\n",
        "def build_model(model_params={}):\n",
        "    args = Munch(model_params)\n",
        "    style_encoder = StyleEncoder(args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim)\n",
        "    return style_encoder\n",
        "\n",
        "def compute_style(speaker_dicts):\n",
        "    reference_embeddings = {}\n",
        "    for key, (path, speaker) in speaker_dicts.items():\n",
        "        wave, sr = librosa.load(path, sr=24000)\n",
        "        audio, index = librosa.effects.trim(wave, top_db=30)\n",
        "        if sr != 24000:\n",
        "            wave = librosa.resample(wave, sr, 24000)\n",
        "        mel_tensor = preprocess(wave).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            label = torch.LongTensor([speaker])\n",
        "            ref = starganv2.style_encoder(mel_tensor.unsqueeze(1), label)\n",
        "        reference_embeddings[key] = (ref, label)\n",
        "    \n",
        "    return reference_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWjcdaVFiHZ"
      },
      "source": [
        "### Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ou4367LCyefA"
      },
      "outputs": [],
      "source": [
        "# load starganv2\n",
        "\n",
        "with open(f'{MODEL_PATH}/config.yml') as f:\n",
        "    starganv2_config = yaml.safe_load(f)\n",
        "starganv2 = build_model(model_params=starganv2_config[\"model_params\"])\n",
        "params = torch.load(f\"{MODEL_PATH}/epoth.pth\", map_location=DEVICE)\n",
        "params = params['model_ema']\n",
        "_ = [starganv2[key].load_state_dict(params[key]) for key in starganv2 if key == \"style_encoder\"]\n",
        "_ = [starganv2[key].eval() for key in starganv2 if key == \"style_encoder\"]\n",
        "starganv2.style_encoder = starganv2.style_encoder.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KvrzOfuFiHc"
      },
      "source": [
        "#### Convert by style encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uf5t5_pIFiHc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# with reference, using style encoder\n",
        "dataframe = pd.read_csv(DATA_PATH, sep=DATA_SEPARETOR, names=DATA_HEADER)\n",
        "\n",
        "speaker_dicts = {}\n",
        "for s in selected_speakers:\n",
        "    print(s)\n",
        "    k = s\n",
        "    speaker_dicts[str(s)] = (f'Demo/VCTK-corpus/{str(k)}/{str(k)}.wav', speakers.index(s))\n",
        "\n",
        "reference_embeddings = compute_style(speaker_dicts)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SWh3o9hvGvJt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
